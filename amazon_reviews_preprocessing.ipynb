{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Amazon Reviews Dataset – Preprocessing, Transformation, and Visualization\n",
        "\n",
        "This notebook performs the following tasks using **only** Python, `pandas`, `matplotlib`, and `seaborn`:\n",
        "\n",
        "1. Data Preprocessing\n",
        "   - Check for missing values in all columns.\n",
        "   - If there are no missing values, introduce 5% missingness in rating and review text.\n",
        "   - Handle missing values via imputation.\n",
        "   - Clean review text (lowercase, remove punctuation, remove stopwords, tokenization).\n",
        "   - Encode categorical variables such as product and user IDs.\n",
        "\n",
        "2. Data Transformation\n",
        "   - Convert review timestamps into datetime objects.\n",
        "   - Derive new features like review year/month and rating categories (positive/negative/neutral).\n",
        "\n",
        "3. Data Visualization\n",
        "   - Visualize the distribution of review ratings.\n",
        "   - Show the trend of number of reviews over time.\n",
        "   - Plot missing value distributions before and after imputation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Setup – Imports & File Path\n",
        "\n",
        "Update `csv_path` to point to your CSV file (e.g. `\"/content/train.csv\"` in Google Colab)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Path to your CSV file\n",
        "csv_path = \"/content/train.csv\"  # TODO: change this if your file is elsewhere\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Inspect Columns and Configure Logical Roles\n",
        "\n",
        "We first load a small sample to see column names and preview the data. Then we map columns to logical roles like rating, text, product ID, user ID, and timestamp.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a small sample to inspect columns\n",
        "sample = pd.read_csv(csv_path, nrows=5)\n",
        "print(\"Columns in the dataset:\")\n",
        "print(sample.columns)\n",
        "sample.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now set the logical column names below. **Edit these strings** to match your actual dataset column names."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Edit these to match your dataset\n",
        "RATING_COL  = \"rating\"      # e.g. \"rating\" or \"Polarity\"\n",
        "TEXT_COL    = \"Text\"        # e.g. \"Text\" or \"review_text\"\n",
        "PRODUCT_COL = \"product_id\"  # set to None if not available\n",
        "USER_COL    = \"user_id\"     # set to None if not available\n",
        "TIME_COL    = \"timestamp\"   # set to None if no time column\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Load the full dataset. If you run into memory errors, you can later switch to chunked processing, but for now we assume it fits into memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\n",
        "    csv_path,\n",
        "    header=None,\n",
        "    engine='python',\n",
        "    on_bad_lines='skip'   # In built Function for Skipping broken lines such as null or \"\"\n",
        ")\n",
        "\n",
        "print(\"Shape of full dataset:\", df.shape)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Check Missing Values in Original Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orig_missing = df.isna().sum()\n",
        "print(\"Missing values in ORIGINAL dataset:\")\n",
        "print(orig_missing)\n",
        "print(\"\\nTotal missing:\", int(orig_missing.sum()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Introduce 5% Missingness (If None Exist)\n",
        "\n",
        "If the dataset has no missing values at all, we randomly introduce 5% missingness into the rating and review text columns to simulate real-world data quality issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "np.random.seed(42)  # for reproducibility\n",
        "\n",
        "# Work on a copy of the original data\n",
        "df_work = df.copy()\n",
        "\n",
        "if orig_missing.sum() == 0:\n",
        "    n = len(df_work)\n",
        "\n",
        "    # 5% masks for rating and text\n",
        "    mask_rating = np.random.rand(n) < 0.05\n",
        "    mask_text   = np.random.rand(n) < 0.05\n",
        "\n",
        "    if RATING_COL in df_work.columns:\n",
        "        df_work.loc[mask_rating, RATING_COL] = np.nan\n",
        "\n",
        "    if TEXT_COL in df_work.columns:\n",
        "        df_work.loc[mask_text, TEXT_COL] = np.nan\n",
        "\n",
        "    print(\"Introduced 5% missingness into rating and text.\")\n",
        "else:\n",
        "    print(\"Dataset already has missing values; not injecting extra.\")\n",
        "\n",
        "missing_before = df_work.isna().sum()\n",
        "print(\"Missing values BEFORE imputation (after possible injection):\")\n",
        "print(missing_before)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Handle Missing Values (Imputation)\n",
        "\n",
        "- Review text: fill with `'missing_review'`.\n",
        "- Rating: fill with dataset median.\n",
        "- Other numeric columns: fill with median.\n",
        "- Other object columns: fill with `'missing_<col>'`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle review text\n",
        "if TEXT_COL in df_work.columns:\n",
        "    df_work[TEXT_COL] = df_work[TEXT_COL].fillna(\"missing_review\")\n",
        "\n",
        "# Handle rating via median imputation\n",
        "if RATING_COL in df_work.columns:\n",
        "    median_rating = df_work[RATING_COL].median()\n",
        "    df_work[RATING_COL] = df_work[RATING_COL].fillna(median_rating)\n",
        "\n",
        "# Other numeric columns\n",
        "num_cols = df_work.select_dtypes(include=[\"number\"]).columns.tolist()\n",
        "if RATING_COL in num_cols:\n",
        "    num_cols.remove(RATING_COL)\n",
        "\n",
        "for col in num_cols:\n",
        "    col_median = df_work[col].median()\n",
        "    df_work[col] = df_work[col].fillna(col_median)\n",
        "\n",
        "# Other object columns\n",
        "obj_cols = df_work.select_dtypes(include=[\"object\"]).columns.tolist()\n",
        "if TEXT_COL in obj_cols:\n",
        "    obj_cols.remove(TEXT_COL)\n",
        "\n",
        "for col in obj_cols:\n",
        "    df_work[col] = df_work[col].fillna(f\"missing_{col}\")\n",
        "\n",
        "missing_after = df_work.isna().sum()\n",
        "print(\"Missing values AFTER imputation:\")\n",
        "print(missing_after)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Clean Review Text (Stopwords, Punctuation, Tokenization)\n",
        "\n",
        "We convert text to lowercase, remove non-letter characters, split into tokens, remove basic English stopwords, and then join the tokens back into a cleaned string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "BASIC_STOPWORDS = {\n",
        "    \"the\", \"is\", \"in\", \"and\", \"a\", \"an\", \"of\", \"to\", \"it\", \"this\",\n",
        "    \"that\", \"i\", \"you\", \"for\", \"on\", \"with\", \"was\", \"are\", \"as\",\n",
        "    \"but\", \"be\", \"have\", \"has\", \"at\", \"or\", \"so\", \"if\"\n",
        "}\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    # keep only letters and spaces\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in BASIC_STOPWORDS]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "if TEXT_COL in df_work.columns:\n",
        "    df_work[\"clean_text\"] = df_work[TEXT_COL].apply(clean_text)\n",
        "    df_work[[TEXT_COL, \"clean_text\"]].head()\n",
        "else:\n",
        "    print(\"TEXT_COL not found; skipping text cleaning.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Encode Categorical Variables (Product ID, User ID)\n",
        "\n",
        "We convert product and user IDs to integer codes using pandas' `category` dtype."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if PRODUCT_COL is not None and PRODUCT_COL in df_work.columns:\n",
        "    df_work[\"product_id_enc\"] = df_work[PRODUCT_COL].astype(\"category\").cat.codes\n",
        "\n",
        "if USER_COL is not None and USER_COL in df_work.columns:\n",
        "    df_work[\"user_id_enc\"] = df_work[USER_COL].astype(\"category\").cat.codes\n",
        "\n",
        "df_work.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Convert Timestamps & Derive New Features\n",
        "\n",
        "We convert the timestamp column to a datetime type (if present), then derive review year, month, and a rating category (`positive`, `neutral`, `negative`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if TIME_COL is not None and TIME_COL in df_work.columns:\n",
        "    df_work[\"review_datetime\"] = pd.to_datetime(df_work[TIME_COL], errors=\"coerce\")\n",
        "    df_work[\"review_year\"] = df_work[\"review_datetime\"].dt.year\n",
        "    df_work[\"review_month\"] = df_work[\"review_datetime\"].dt.month\n",
        "else:\n",
        "    print(\"No valid TIME_COL configured; skipping datetime features.\")\n",
        "\n",
        "def rating_category(x):\n",
        "    try:\n",
        "        x = float(x)\n",
        "    except:\n",
        "        return \"unknown\"\n",
        "    if x >= 4:\n",
        "        return \"positive\"\n",
        "    elif x <= 2:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "if RATING_COL in df_work.columns:\n",
        "    df_work[\"rating_category\"] = df_work[RATING_COL].apply(rating_category)\n",
        "    df_work[[RATING_COL, \"rating_category\"]].head()\n",
        "else:\n",
        "    print(\"No RATING_COL; skipping rating categories.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Visualization – Distribution of Review Ratings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if RATING_COL in df_work.columns:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.countplot(x=RATING_COL, data=df_work)\n",
        "    plt.title(\"Distribution of Review Ratings\")\n",
        "    plt.xlabel(\"Rating\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No RATING_COL to plot distribution.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Visualization – Trend of Number of Reviews Over Time\n",
        "\n",
        "We plot the monthly count of reviews over time if datetime information is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if \"review_datetime\" in df_work.columns:\n",
        "    df_work[\"year_month\"] = df_work[\"review_datetime\"].dt.to_period(\"M\")\n",
        "    trend = df_work.groupby(\"year_month\").size().reset_index(name=\"review_count\")\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(trend[\"year_month\"].astype(str), trend[\"review_count\"], marker=\"o\")\n",
        "    plt.xticks(rotation=60)\n",
        "    plt.title(\"Number of Reviews Over Time (Monthly)\")\n",
        "    plt.xlabel(\"Year-Month\")\n",
        "    plt.ylabel(\"Number of Reviews\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"No 'review_datetime' column; cannot plot trend over time.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualization – Missing Values Before vs After Imputation\n",
        "\n",
        "We compare the missing value counts per column before and after imputation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_df = pd.DataFrame({\n",
        "    \"column\": missing_before.index,\n",
        "    \"before\": missing_before.values,\n",
        "    \"after\": missing_after.values\n",
        "})\n",
        "\n",
        "missing_long = missing_df.melt(id_vars=\"column\", value_vars=[\"before\", \"after\"],\n",
        "                               var_name=\"stage\", value_name=\"missing_count\")\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(data=missing_long, x=\"column\", y=\"missing_count\", hue=\"stage\")\n",
        "plt.title(\"Missing Values Before vs After Imputation\")\n",
        "plt.xticks(rotation=45, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
